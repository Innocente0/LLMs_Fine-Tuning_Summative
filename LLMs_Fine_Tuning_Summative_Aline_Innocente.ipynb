{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Innocente0/LLMs_Fine-Tuning_Summative/blob/main/LLMs_Fine_Tuning_Summative_Aline_Innocente.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atH2YPhzS8kM"
      },
      "source": [
        "#Medical Assistant via LLMs Fine-Tuning\n",
        "This cell checks if a GPU is available in Colab and shows its type and memory. It helps confirm that fine-tuning can run efficiently on available hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "V0bBXYzlVnk8"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4hl3lSxzPOh"
      },
      "source": [
        "**Install Dependencies**\n",
        "\n",
        "Installs all required libraries (Transformers, PEFT, BitsAndBytes, Datasets, Gradio, etc.). This ensures the Colab environment has everything needed for loading, fine-tuning, evaluating, and deploying the LLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsclkpsRm3lw"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U transformers datasets accelerate peft trl bitsandbytes evaluate rouge_score nltk sentencepiece gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ1LMClfzq7R"
      },
      "source": [
        "**Imports and NLTK Download**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkDojIaum9D_"
      },
      "outputs": [],
      "source": [
        "import os, time, math, random, zipfile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig, TrainingArguments\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import evaluate\n",
        "import nltk\n",
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALZSOjsbosdT"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSCdQRLNnx8c"
      },
      "outputs": [],
      "source": [
        "ZIP_PATH = \"/content/medquad.csv.zip\"  # your uploaded file\n",
        "EXTRACT_DIR = \"/mnt/data/medquad_extracted\"\n",
        "\n",
        "os.makedirs(EXTRACT_DIR, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(ZIP_PATH, \"r\") as z:\n",
        "    z.extractall(EXTRACT_DIR)\n",
        "\n",
        "print(\"Extracted files:\", os.listdir(EXTRACT_DIR))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmuwSSD-1DQJ"
      },
      "source": [
        "Loads the extracted MedQuAD CSV into a pandas DataFrame and prints its shape. This is for inspecting columns and cleaning the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IEvsbC5n_uZ"
      },
      "outputs": [],
      "source": [
        "CSV_PATH = os.path.join(EXTRACT_DIR, \"medquad.csv\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_2dyGh_o44H"
      },
      "source": [
        "**Clean and Filter Dataset**\n",
        "Removes missing values, duplicates, and extremely short questionâ€“answer pairs, then normalizes text. This improves data quality and ensures the model trains on meaningful, consistent medical examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCwaIbpcoOb7"
      },
      "outputs": [],
      "source": [
        "# Basic cleanup\n",
        "df = df.dropna(subset=[\"question\", \"answer\"]).copy()\n",
        "df[\"question\"] = df[\"question\"].astype(str).str.strip()\n",
        "df[\"answer\"] = df[\"answer\"].astype(str).str.strip()\n",
        "\n",
        "# Remove empties\n",
        "df = df[(df[\"question\"].str.len() > 5) & (df[\"answer\"].str.len() > 10)]\n",
        "\n",
        "# Drop duplicates\n",
        "df = df.drop_duplicates(subset=[\"question\", \"answer\"])\n",
        "\n",
        "print(\"After cleaning:\", df.shape)\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXjKQqMc2jzw"
      },
      "source": [
        "Summarized the datasets from 16k to 3k samples, with questions and answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwQd7zeapDwR"
      },
      "outputs": [],
      "source": [
        "# 3000 samples questions and answers\n",
        "TRAIN_SIZE = 3000\n",
        "\n",
        "if TRAIN_SIZE < len(df):\n",
        "    df = df.sample(TRAIN_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Using dataset size:\", len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ3M8-jGpWp2"
      },
      "source": [
        "**Formatting the Datasets into Instruction-Response Template**\n",
        "\n",
        "Converts each row into a single formatted string with **Instruction** and **Response** sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L13voJTvpMNA"
      },
      "outputs": [],
      "source": [
        "def format_example(row):\n",
        "    topic = row.get(\"focus_area\", \"\")\n",
        "    topic_str = f\"[Topic: {topic}] \" if isinstance(topic, str) and len(topic.strip()) > 0 else \"\"\n",
        "    return (\n",
        "        \"### Instruction:\\n\"\n",
        "        f\"{topic_str}{row['question']}\\n\\n\"\n",
        "        \"### Response:\\n\"\n",
        "        f\"{row['answer']}\"\n",
        "    )\n",
        "\n",
        "df[\"text\"] = df.apply(format_example, axis=1)\n",
        "df[[\"question\", \"answer\", \"focus_area\", \"text\"]].head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyHs-4uDp5fl"
      },
      "source": [
        "**Train and Validation Split**\n",
        "Converts the DataFrame to a Hugging Face Dataset and splits it into training and validation subsets. This enables proper evaluation of fine-tuning performance on unseen examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkKaN7FypgvM"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset.from_pandas(df[[\"text\"]])\n",
        "\n",
        "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds = split[\"train\"]\n",
        "eval_ds  = split[\"test\"]\n",
        "\n",
        "print(train_ds, eval_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIDgIdHJqFgc"
      },
      "source": [
        "**Load Base Model and Tokenizer**\n",
        "\n",
        "Configures TinyLlama 1.1B in 4-bit with tokenizer and pad token, fitting the model into Colab GPU memory while enabling efficient tokenization and LoRA fine-tuning for domain-specific medical text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkd7awZUp9Qz"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # important for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDyp8Tk0qvcR"
      },
      "source": [
        "**Adding LoRA Adapters**\n",
        "\n",
        "Prepares the quantized model for k-bit training and inserts LoRA adapters into attention projections. This enables parameter-efficient fine-tuning while freezing most original model weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-U6H5pt_qJ6l"
      },
      "outputs": [],
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # TinyLlama/Llama-like blocks usually use these:\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSI2zMrPrgEd"
      },
      "source": [
        "**Training Setup and Experiments Tracking Table**\n",
        "\n",
        "Defines a function to append hyperparameters and metric results into a CSV file. This creates an experiment log for tracking different runs and comparing performance systematically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Zn8ZR8Uqy5i"
      },
      "outputs": [],
      "source": [
        "EXPERIMENTS_CSV = \"/mnt/data/experiments_log.csv\"\n",
        "\n",
        "def log_experiment(row_dict):\n",
        "    df_log = pd.DataFrame([row_dict])\n",
        "    if os.path.exists(EXPERIMENTS_CSV):\n",
        "        old = pd.read_csv(EXPERIMENTS_CSV)\n",
        "        df_log = pd.concat([old, df_log], ignore_index=True)\n",
        "    df_log.to_csv(EXPERIMENTS_CSV, index=False)\n",
        "    return df_log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA7Ikez8ODfy"
      },
      "source": [
        "Tokenizes training text, pads batches with a collator that sets labels from inputs, and defines learning and logging hyperparameters, preparing token IDs and stable loss for causal language model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kFCMZ669rqRh"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "\n",
        "LR = 5e-5\n",
        "BATCH_SIZE = 2\n",
        "GRAD_ACCUM = 8\n",
        "EPOCHS = 2\n",
        "MAX_SEQ_LEN = 512\n",
        "\n",
        "# Tokenize WITHOUT labels (important)\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"text\"],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        padding=False,  # we pad in the collator\n",
        "    )\n",
        "\n",
        "train_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=train_ds.column_names)\n",
        "eval_tok  = eval_ds.map(tokenize_fn, batched=True, remove_columns=eval_ds.column_names)\n",
        "\n",
        "# Custom collator: pads then creates labels = input_ids, and masks pad tokens\n",
        "base_pad_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True)\n",
        "\n",
        "def causal_lm_collator(features):\n",
        "    batch = base_pad_collator(features)  # pads input_ids + attention_mask\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "    labels[batch[\"attention_mask\"] == 0] = -100  # ignore padding in loss\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch\n",
        "\n",
        "# Training arguments (your transformers uses eval_strategy)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/mnt/data/medquad_tinyllama_lora\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    learning_rate=LR,\n",
        "    warmup_steps=10,\n",
        "    logging_steps=20,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tok,\n",
        "    eval_dataset=eval_tok,\n",
        "    data_collator=causal_lm_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCYHXd6GvD-B"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "train_result = trainer.train()\n",
        "train_time_sec = time.time() - start_time\n",
        "\n",
        "train_time_sec, train_result.metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKE0sI4qvLcR"
      },
      "source": [
        "**Save Fine-Tuned Adapter and Tokenizer**\n",
        "\n",
        "Saves the fine-tuned LoRA adapter weights and tokenizer configuration to disk. This allows reloading the specialized medical assistant later without retraining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAi_iO6FrsuT"
      },
      "outputs": [],
      "source": [
        "SAVE_DIR = \"/mnt/data/medquad_tinyllama_lora_adapter\"\n",
        "trainer.model.save_pretrained(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(\"Saved to:\", SAVE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CVTBh3xvkFu"
      },
      "source": [
        "**Helper: generate_response Function**\n",
        "\n",
        "Defines a generation helper that tokenizes prompts, calls model.generate, and decodes outputs. It optionally extracts only the part after **Response** for clean responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSFkbyvxu4zo"
      },
      "outputs": [],
      "source": [
        "def generate_response(model_obj, prompt, max_new_tokens=160, temperature=0.7, top_p=0.9):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SEQ_LEN).to(model_obj.device)\n",
        "    with torch.no_grad():\n",
        "        out = model_obj.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "    # return only the part after \"### Response:\" if present\n",
        "    if \"### Response:\" in text:\n",
        "        return text.split(\"### Response:\", 1)[-1].strip()\n",
        "    return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8tThRp0v0xD"
      },
      "source": [
        "**Build a Small Eval for Metrics**\n",
        "\n",
        "Constructs prompts and reference answers from the validation dataset by splitting **Instruction** and **Response**. These are used to fairly compare base and fine-tuned models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haaEaDG4v-CE"
      },
      "outputs": [],
      "source": [
        "# 100 samples for evaluation metrics\n",
        "EVAL_N = min(100, len(eval_ds))\n",
        "eval_samples = eval_ds.select(range(EVAL_N))\n",
        "\n",
        "# Extract references (true answers) from formatted text\n",
        "def extract_ref(formatted_text):\n",
        "    if \"### Response:\" in formatted_text:\n",
        "        return formatted_text.split(\"### Response:\", 1)[-1].strip()\n",
        "    return formatted_text\n",
        "\n",
        "def extract_prompt(formatted_text):\n",
        "# prompt is everything up to Response\n",
        "    if \"### Response:\" in formatted_text:\n",
        "        return formatted_text.split(\"### Response:\", 1)[0].strip() + \"\\n\\n### Response:\\n\"\n",
        "    return formatted_text.strip() + \"\\n\\n### Response:\\n\"\n",
        "\n",
        "prompts = [extract_prompt(x[\"text\"]) for x in eval_samples]\n",
        "refs = [extract_ref(x[\"text\"]) for x in eval_samples]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30in-dilwGNl"
      },
      "source": [
        "**Load Base Model for Comparison**\n",
        "\n",
        "Reloads the original TinyLlama base model in 4-bit quantized form. This enables direct side-by-side comparison between the untrained base model and the fine-tuned medical assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0xZj03gwJ6H"
      },
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "base_model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hhVDrpiwNMJ"
      },
      "source": [
        "**Generate Predictions**\n",
        "\n",
        "Uses both base and fine-tuned models to generate answers for the same validation prompts. These predictions are later evaluated with ROUGE, BLEU, and perplexity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7akQ9H_wQAa"
      },
      "outputs": [],
      "source": [
        "ft_preds = [generate_response(trainer.model, p) for p in prompts]\n",
        "base_preds = [generate_response(base_model, p) for p in prompts]\n",
        "\n",
        "print(\"Sample prompt:\\n\", prompts[0])\n",
        "print(\"\\nBASE:\\n\", base_preds[0][:400])\n",
        "print(\"\\nFINE-TUNED:\\n\", ft_preds[0][:400])\n",
        "print(\"\\nREF:\\n\", refs[0][:400])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27ipMafWwTdc"
      },
      "source": [
        "**Compute ROUGE and BLEU Scores**\n",
        "\n",
        "Loads ROUGE and BLEU metrics, computes them for base and fine-tuned predictions versus references, and returns metric dictionaries. This quantifies lexical and structural similarity improvements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D_tRWoPwWOX"
      },
      "outputs": [],
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(preds, refs):\n",
        "    # ROUGE expects raw strings for predictions and references\n",
        "    rouge_scores = rouge.compute(predictions=preds, references=refs)\n",
        "    # BLEU expects raw strings for predictions and a list of raw strings for references (per prediction)\n",
        "    bleu_scores = bleu.compute(predictions=preds, references=[[r] for r in refs])\n",
        "    return rouge_scores, bleu_scores\n",
        "\n",
        "ft_rouge, ft_bleu = compute_metrics(ft_preds, refs)\n",
        "base_rouge, base_bleu = compute_metrics(base_preds, refs)\n",
        "\n",
        "ft_rouge, ft_bleu, base_rouge, base_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl3wOMYDweXS"
      },
      "source": [
        "**Compute Perplexity on Validation Texts**\n",
        "\n",
        "Calculates perplexity for base and fine-tuned models on validation examples using language model loss. Lower perplexity indicates the fine-tuned model better models medical text distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3hQnactwhho"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def perplexity_on_texts(model_obj, texts, max_len=512):\n",
        "    model_obj.eval()\n",
        "    losses = []\n",
        "    for t in texts:\n",
        "        enc = tokenizer(t, return_tensors=\"pt\", truncation=True, max_length=max_len).to(model_obj.device)\n",
        "        with torch.no_grad():\n",
        "            out = model_obj(**enc, labels=enc[\"input_ids\"])\n",
        "            losses.append(out.loss.item())\n",
        "    return float(math.exp(np.mean(losses)))\n",
        "\n",
        "# Evaluate perplexity on the formatted eval examples\n",
        "eval_texts = [x[\"text\"] for x in eval_samples]\n",
        "ft_ppl = perplexity_on_texts(trainer.model, eval_texts, max_len=MAX_SEQ_LEN)\n",
        "base_ppl = perplexity_on_texts(base_model, eval_texts, max_len=MAX_SEQ_LEN)\n",
        "\n",
        "ft_ppl, base_ppl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTEGtPZiwm8A"
      },
      "source": [
        "**Log Experiments Row**\n",
        "\n",
        "Collects hyperparameters, metrics, training time, and GPU memory usage into a dictionary and appends it to the experiment log CSV. This documents the final best run clearly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPAymNHfwpX4"
      },
      "outputs": [],
      "source": [
        "gpu_mem = torch.cuda.max_memory_allocated() / (1024**3) if torch.cuda.is_available() else None\n",
        "\n",
        "exp_row = {\n",
        "    \"model\": MODEL_ID,\n",
        "    \"dataset\": \"MedQuAD\",\n",
        "    \"train_size\": len(train_ds),\n",
        "    \"eval_size\": len(eval_ds),\n",
        "    \"max_seq_len\": MAX_SEQ_LEN,\n",
        "    \"lr\": LR,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"grad_accum\": GRAD_ACCUM,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"lora_r\": lora_config.r,\n",
        "    \"lora_alpha\": lora_config.lora_alpha,\n",
        "    \"lora_dropout\": lora_config.lora_dropout,\n",
        "    \"train_time_sec\": train_time_sec,\n",
        "    \"gpu_max_mem_gb\": gpu_mem,\n",
        "    \"ft_rougeL\": ft_rouge.get(\"rougeL\", None),\n",
        "    \"base_rougeL\": base_rouge.get(\"rougeL\", None),\n",
        "    \"ft_bleu\": ft_bleu.get(\"bleu\", None),\n",
        "    \"base_bleu\": base_bleu.get(\"bleu\", None),\n",
        "    \"ft_ppl\": ft_ppl,\n",
        "    \"base_ppl\": base_ppl,\n",
        "    \"notes\": \"LoRA 4-bit TinyLlama, MedQuAD formatted template\"\n",
        "}\n",
        "\n",
        "log_experiment(exp_row).tail(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSFWo0JSwsqc"
      },
      "source": [
        "**Qualitative Comparison Table**\n",
        "\n",
        "Runs both base and fine-tuned models on a small set of custom medical and non-medical questions, then builds a comparison DataFrame. This showcases behavioral differences beyond numeric metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrXtBQUfwzE-"
      },
      "outputs": [],
      "source": [
        "def compare_on_questions(questions):\n",
        "    rows = []\n",
        "    for q in questions:\n",
        "        prompt = f\"### Instruction:\\n{q}\\n\\n### Response:\\n\"\n",
        "        base_ans = generate_response(base_model, prompt)\n",
        "        ft_ans = generate_response(trainer.model, prompt)\n",
        "        rows.append({\"question\": q, \"base_answer\": base_ans, \"fine_tuned_answer\": ft_ans})\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "test_questions = [\n",
        "    \"What are common symptoms of anemia?\",\n",
        "    \"How is hypertension typically treated?\",\n",
        "    \"What causes asthma?\",\n",
        "    \"What is the recommended action for a high fever in a child?\",\n",
        "    # out-of-domain checks\n",
        "    \"Write a Python function to sort a list.\",\n",
        "    \"Who won the 2022 World Cup?\"\n",
        "]\n",
        "\n",
        "compare_df = compare_on_questions(test_questions)\n",
        "compare_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTaUVke5w2c-"
      },
      "source": [
        "**Gradio Chatbot Interface and Deployment**\n",
        "\n",
        "Defines the chat function with disclaimer and safety formatting, then builds and launches a Gradio Interface. This provides an interactive web UI for users to test the medical assistant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Yt-P7uw5s-"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat(user_question):\n",
        "    prompt = f\"### Instruction:\\n{user_question}\\n\\n### Response:\\n\"\n",
        "    answer = generate_response(trainer.model, prompt, max_new_tokens=220, temperature=0.7, top_p=0.9)\n",
        "    return answer\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=chat,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Ask a medical question...\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Medical Q&A Assistant (MedQuAD fine-tuned with LoRA)\",\n",
        "    description=\"Domain-specific assistant fine-tuned from TinyLlama using MedQuAD. For education/demo only.\"\n",
        ")\n",
        "\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mo30zPnBSApf"
      },
      "source": [
        "**Save to Google Drive**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud3lzUr61muG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "VcRMJ1yF9QFq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "DRIVE_DIR = \"/content/drive/MyDrive/medquad_lora_run\"\n",
        "os.makedirs(DRIVE_DIR, exist_ok=True)\n",
        "\n",
        "!cp -r \"{SAVE_DIR}\" \"{DRIVE_DIR}/adapter\"\n",
        "!cp \"{EXPERIMENTS_CSV}\" \"{DRIVE_DIR}/experiments_log.csv\"\n",
        "print(\"Saved adapters + logs to:\", DRIVE_DIR)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO5WluwIm4mWYGIWJA0NU+e",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}